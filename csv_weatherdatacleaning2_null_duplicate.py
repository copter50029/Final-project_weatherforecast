# -*- coding: utf-8 -*-
"""CSV WeatherDataCleaning2 Null duplicate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ODnwuQaL7ZckVLa3ODHHIAV8Gkdzjas6
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, trim, upper, when, count

def clean_csv(input_path, output_path, district_column="district"):
    """
    Cleans a CSV file using PySpark, reporting null counts before and after cleaning,
    and orders the data by the specified district column.

    Args:
        input_path (str): Path to the input CSV file.
        output_path (str): Path to save the cleaned CSV file.
        district_column (str): Name of the column to order by (default: "district").
    """

    spark = SparkSession.builder.appName("CSVCleaner").getOrCreate()

    try:
        df = spark.read.csv(input_path, header=True, inferSchema=True)

        original_columns = df.columns

        print("Null Counts Before Cleaning:")
        null_counts_before = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])
        null_counts_before.show()

        df = df.dropna()
        df = df.dropDuplicates()

        for column, dtype in df.dtypes:
            if dtype == 'string':
                df = df.withColumn(column, trim(col(column)))
                df = df.withColumn(column, upper(col(column)))

        for column, dtype in df.dtypes:
            if dtype == 'string':
                df = df.withColumn(column, when(col(column) == "", "Unknown").otherwise(col(column)))

        print("\nNull Counts After Cleaning:")
        null_counts_after = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])
        null_counts_after.show()

        # Order by the district column
        if district_column in df.columns:
            df = df.orderBy(col(district_column))
        else:
            print(f"Warning: Column '{district_column}' not found. Data will not be ordered.")

        df = df.select(original_columns)

        df.write.csv(output_path, header=True, mode="overwrite")

        print(f"Cleaned CSV saved to: {output_path}")

    except Exception as e:
        print(f"An error occurred: {e}")

    finally:
        spark.stop()

if __name__ == "__main__":
    input_csv_path = "weather_data.csv"
    output_csv_path = "cleaned.csv"
    district_column_name = "district" #change this to your district column name if different.
    clean_csv(input_csv_path, output_csv_path, district_column_name)